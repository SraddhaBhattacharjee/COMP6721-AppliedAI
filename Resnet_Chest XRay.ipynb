{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naOl1p_8N5kl","executionInfo":{"status":"ok","timestamp":1678837942204,"user_tz":240,"elapsed":16063,"user":{"displayName":"Sraddha Project","userId":"10376738432179747804"}},"outputId":"bb52d5f2-a712-4b5f-c709-d3ea4d9704bb"},"id":"naOl1p_8N5kl","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"id":"b9b3415d","metadata":{"id":"b9b3415d","executionInfo":{"status":"ok","timestamp":1678838873794,"user_tz":240,"elapsed":213,"user":{"displayName":"Sraddha Project","userId":"10376738432179747804"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e7968cd-fad9-44bd-8644-256c5e5aaead"},"outputs":[{"output_type":"stream","name":"stdout","text":["importing Jupyter notebook from utils.ipynb\n"]}],"source":["import os\n","import torch\n","import random\n","import numpy as np\n","from PIL import Image\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import precision_score, recall_score, f1_score"]},{"cell_type":"code","source":["data_path = \"drive/My Drive/Chest XRay Images/\"\n","sample_ratio = 1"],"metadata":{"id":"4OHDnzINmaWL"},"id":"4OHDnzINmaWL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","random.seed(42)\n","\n","# Get dataset from folder\n","dataset = datasets.ImageFolder(root = data_path, transform = data_transforms)\n","\n","# Create data transforms\n","data_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","# Get 30% of the data randomly\n","num_data = len(dataset)\n","num_sample = int(num_data * sample_ratio)\n","indices = np.random.choice(range(num_data), num_sample, replace=False)\n","\n","# Split the data into training, test, and validation sets\n","num_train = int(num_sample * 0.7)\n","num_test = int(num_sample * 0.2)\n","num_val = num_sample - num_train - num_test\n","\n","train_indices = indices[:num_train]\n","test_indices = indices[num_train:num_train+num_test]\n","val_indices = indices[num_train+num_test:]\n","\n","train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n","test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n","val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n","\n","# Create data loaders for training, test, and validation sets\n","batch_size = 32\n","\n","train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n","test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n","val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n","\n","print(train_loader, num_sample, num_train)"],"metadata":{"id":"m8ijtwkIpZhs"},"id":"m8ijtwkIpZhs","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"adbe3fa8","metadata":{"id":"adbe3fa8","outputId":"29cd9c9e-d7c7-45ef-ff88-4a64dc931a9c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678831358224,"user_tz":240,"elapsed":12263011,"user":{"displayName":"Sraddha Project","userId":"10376738432179747804"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.9.0\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1, Training Loss: 0.2616\n","Epoch: 2, Training Loss: 0.1624\n","Epoch: 3, Training Loss: 0.1333\n","Epoch: 4, Training Loss: 0.1204\n","Epoch: 5, Training Loss: 0.1056\n","Epoch: 6, Training Loss: 0.0946\n","Epoch: 7, Training Loss: 0.1150\n","Epoch: 8, Training Loss: 0.0832\n","Epoch: 9, Training Loss: 0.0755\n","Epoch: 10, Training Loss: 0.0862\n","Test Loss: 0.2806, Test Recall: 0.8322, Test Precision: 0.9405, Test F-score: 0.8690\n","Validation Loss: 0.2470, Validation Recall: 0.8125, Validation Precision: 0.9456, Validation F-score: 0.8558\n"]}],"source":["# Define ResNet18 model\n","model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=False)\n","num_classes = len(dataset.classes)\n","model.fc = nn.Linear(512, num_classes)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Train model on training set\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    train_loss = 0.0\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * inputs.size(0)\n","    train_loss /= len(train_indices)\n","    print('Epoch: {}, Training Loss: {:.4f}'.format(epoch+1, train_loss))\n","\n","# Evaluate model on test set\n","test_loss = 0.0\n","test_pred = []\n","test_true = []\n","model.eval()\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        test_loss += loss.item() * inputs.size(0)\n","        _, predicted = torch.max(outputs.data, 1)\n","        test_pred.extend(predicted.cpu().numpy())\n","        test_true.extend(labels.cpu().numpy())\n","\n","test_loss /= len(test_indices)\n","test_recall = recall_score(test_true, test_pred, average='macro')\n","test_precision = precision_score(test_true, test_pred, average='macro')\n","test_fscore = f1_score(test_true, test_pred, average='macro')\n","\n","print('Test Loss: {:.4f}, Test Recall: {:.4f}, Test Precision: {:.4f}, Test F-score: {:.4f}'.format(test_loss, test_recall, test_precision, test_fscore))\n","\n","# Evaluate model on validation set\n","val_loss = 0.0\n","val_pred = []\n","val_true = []\n","model.eval()\n","\n","with torch.no_grad():\n","    for inputs, labels in val_loader:\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        val_loss += loss.item() * inputs.size(0)\n","        _, predicted = torch.max(outputs.data, 1)\n","        val_pred.extend(predicted.cpu().numpy())\n","        val_true.extend(labels.cpu().numpy())\n","\n","val_loss /= len(val_indices)\n","val_recall = recall_score(val_true, val_pred, average='macro')\n","val_precision = precision_score(val_true, val_pred, average='macro')\n","val_fscore = f1_score(val_true, val_pred, average='macro')\n","\n","print('Validation Loss: {:.4f}, Validation Recall: {:.4f}, Validation Precision: {:.4f}, Validation F-score: {:.4f}'.format(val_loss, val_recall, val_precision, val_fscore))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[{"file_id":"1jdiRcofG1UHTLNMw49Gtur7mCEfPNZAN","timestamp":1678839876835}]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}