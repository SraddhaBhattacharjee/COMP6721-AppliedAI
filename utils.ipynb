{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Load the Dataset and convert from .npz to .jpg\n",
        "```\n",
        "npz = np.load(\"Dataset5_raw_val.npz\")\n",
        "image_data = npz['image']\n",
        "image_names = npz['image_name']\n",
        "image_labels = npz['image_label']\n",
        "\n",
        "for i in range(len(image_data)):\n",
        "    image_array = image_data[i]\n",
        "    image = Image.fromarray(np.uint8(image_array))\n",
        "    \n",
        "    image_name = image_names[i][0]\n",
        "    image_label = np.uint8(image_labels[i][0])\n",
        "    \n",
        "    filename = f\"{image_label}_{image_name}.jpg\"\n",
        "    image.save(f\"val_data/{filename}\")\n",
        "```\n",
        "\n",
        "# Set directory where files are stored\n",
        "\n",
        "```\n",
        "directory = 'new_train/'\n",
        "```\n",
        "\n",
        "# Get a list of all files in the directory\n",
        "\n",
        "```\n",
        "files = os.listdir(directory)\n",
        "\n",
        "for i, filename in enumerate(files):\n",
        "    # Split the filename into its parts\n",
        "    name, ext = os.path.splitext(filename)\n",
        "    label, img_name = name.split('_', 1)\n",
        "\n",
        "    # Pad image name with zeros to make it 3 digits long\n",
        "    img_name = str(i).zfill(5)\n",
        "\n",
        "    # Construct new filename\n",
        "    new_filename = f\"{img_name}.jpg\"\n",
        "\n",
        "    # Rename the file\n",
        "    os.rename(os.path.join(directory, filename), os.path.join(f\"data/{label}\", new_filename))\n",
        "```"
      ],
      "metadata": {
        "id": "JTZHJfQloMhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix"
      ],
      "metadata": {
        "id": "WV88v66a3aYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_sampling(indices):\n",
        "  return torch.utils.data.sampler.SubsetRandomSampler(indices)"
      ],
      "metadata": {
        "id": "rzfbbPVRfHNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing\n",
        "def data_preprocess(data_path, sample_ratio):\n",
        "  # Create data transforms\n",
        "  data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "  # Get dataset from folder and apply data transforms\n",
        "  dataset = datasets.ImageFolder(root = \"{}data\".format(data_path), transform = data_transforms)\n",
        "    \n",
        "  # Get a sample of the data randomly\n",
        "  num_samples = int(len(dataset) * sample_ratio)\n",
        "  indices = np.random.choice(range(len(dataset)), num_samples, replace = False)\n",
        "\n",
        "  # Split the data into training, test, and validation sets\n",
        "  train_size = int(0.7 * num_samples)\n",
        "  test_size = int(0.2 * num_samples)\n",
        "  val_size = num_samples - train_size - test_size\n",
        "\n",
        "  train_indices = indices[ : train_size]\n",
        "  test_indices = indices[train_size : train_size + test_size]\n",
        "  val_indices = indices[train_size + test_size : ]\n",
        "\n",
        "  samples = [data_sampling(i) for i in [train_indices, test_indices, val_indices]]\n",
        "\n",
        "  # Create data loaders for training, test, and validation sets\n",
        "  train_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[0])\n",
        "  test_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[1])\n",
        "  val_loader = DataLoader(dataset, batch_size = batch_size, sampler = samples[2])\n",
        "\n",
        "  return dataset, train_loader, train_indices, test_loader, test_indices, val_loader, val_indices"
      ],
      "metadata": {
        "id": "sSFeMnrKeyo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, data_size, dtype, criterion, data_path, model_name):\n",
        "  _loss, _pred, _true = 0.0, [], []\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in dataloader:\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          _loss += loss.item() * inputs.size(0)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          _pred.extend(predicted.cpu().numpy())\n",
        "          _true.extend(labels.cpu().numpy())\n",
        "  \n",
        "  _loss /= len(data_size)\n",
        "  _recall = recall_score(_true, _pred, average='macro')\n",
        "  _precision = precision_score(_true, _pred, average='macro')\n",
        "  _fscore = f1_score(_true, _pred, average='macro')\n",
        "\n",
        "  print('MODEL PERFORMANCE ON {} SET'.format(dtype))\n",
        "  print('Loss: {:.4f}, Recall: {:.4f}, Precision: {:.4f}, F-score: {:.4f}'.format(_loss, _recall, _precision, _fscore))\n",
        "  print(\"\")\n",
        "\n",
        "  cm = confusion_matrix(_true, _pred)\n",
        "  plt.figure(figsize = (8, 8))\n",
        "  plt.imshow(cm, cmap = plt.cm.Blues)\n",
        "  plt.title(\"{}_{}SET_CONFUSION_MATRIX\".format(model_name, dtype))\n",
        "  plt.colorbar()\n",
        "  plt.savefig(\"{}_{}SET_CONFUSION_MATRIX.png\".format(model_name, dtype))"
      ],
      "metadata": {
        "id": "gIfwoRhe1zF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}